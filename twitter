import snscrape.modules.twitter as snmtwitter
import streamlit as st
import pandas as pd
import pymongo
from bson.json_util import dumps

client = pymongo.MongoClient(
    "mongodb+srv://vinoth:122385@cluster0.tz2cqxj.mongodb.net/?retryWrites=true&w=majority")
db = client.demo
collections = db.sample


def upload_to_mongodb(pdDf, textRes, startDate, endDate):
    if st.button("Upload the data"):
        docUpload = {}
        print(pdDf)
        convertPdDfToDt = pdDf.to_dict('records')
        print(convertPdDfToDt)
        docUpload["Scraped Word"] = textRes
        docUpload["Scrapped Start Date"] = startDate.isoformat()
        docUpload["Scrapped End Date"] = endDate.isoformat()
        docUpload["Scrapped Data"] = convertPdDfToDt
        collections.insert_one(docUpload)
        st.write("Successfully data uploaded into MongoDB")


def twitter_search_scraper(searchOptions, txtRes, count, startDate, endDate):
    tweetsList = []
    if searchOptions == "Keyword":
        for i, tweets in enumerate(snmtwitter.TwitterSearchScraper("from:" + txtRes).get_items()):
            if i > count:
                break
            tweetsList.append([tweets.date, tweets.id, tweets.content, tweets.user.username,
                               tweets.replyCount, tweets.retweetCount, tweets.lang,
                               tweets.source, tweets.likeCount])
    else:
        for i, tweets in enumerate(snmtwitter.TwitterSearchScraper(
                txtRes + " since:" + str(startDate) + " until:" + str(endDate)).get_items()):
            if i > count:
                break
            tweetsList.append([tweets.date, tweets.id, tweets.content, tweets.user.username,
                               tweets.replyCount, tweets.retweetCount, tweets.lang,
                               tweets.source, tweets.likeCount])
    return tweetsList


def sidebar_select_box(label, ls):
    sideSelRes = st.sidebar.selectbox(label, ls)
    return sideSelRes


def select_box(label, ls):
    selRes = st.selectbox(label, ls)
    return selRes


def txt_input(options):
    txtRes = st.text_input(options, "")
    # print(txtRes)
    return txtRes


def write(label, val):
    st.write(label, val)


def num_input(label):
    numRes = st.number_input(label)
    return numRes


def date_selection(label):
    selDate = st.date_input(label)
    return selDate


def convert_pandasdf_to_stdf(df):
    stDataFrame = st.dataframe(df)
    return stDataFrame


def pandas_df(ls, columnnames):
    pdDataFrame = pd.DataFrame(ls, columns=columnnames)
    return pdDataFrame


def scrap_data(tweetList, columnnames):
    global pdData, stData
    if st.button("Scrap the data for the given input"):
        pdData = pandas_df(tweetList, columnnames)
        stData = convert_pandasdf_to_stdf(pdData)
    return pdData, stData


def check_avl_data():
    valuesList = []
    for i in collections.find({}, {"Scraped Word": 1, "_id": 0}):
        key, val = next(iter(i.items()))
        valuesList.append(val)
    dataList = list(set(valuesList))
    retrieveOption = select_box("List out available keyword and hashtag stored already", dataList)
    return retrieveOption


def display_data(retrieveValue):
    global displayDoc
    for i in collections.find({"Scraped Word": retrieveValue}):
        displayDoc = i
    if st.button("Display the data for given scraped word"):
        write("", displayDoc)


def download_data(retrieveValue):
    global displayDoc
    for j in collections.find({"Scraped Word": retrieveValue}):
        displayDoc = j
    jsonData = dumps(displayDoc, indent=2)
    csvData = pd.DataFrame(displayDoc)
    csvData.pop("_id")
    csvData = csvData.to_csv(index=False).encode('utf-8')
    st.download_button(label="Download data as JSON", data=jsonData,
                       file_name="TwitterScrapData.json")
    st.download_button(label="Download data as CSV", data=csvData, file_name="TwitterScrapData.csv",
                       mime="csv")


def main():
    operationLists = ["Search", "Display", "Download"]
    searchOptionsLists = ["Keyword", "Hashtag"]
    columnnames = ["Datetime", "Tweet Id", "Text", "Username", "ReplyCount", "RetweetCount",
                   "Language", "Source", "LikeCount"]
    sidebarSelResult = sidebar_select_box("Which operation you want to perform", operationLists)

    if sidebarSelResult == "Search":
        searchOptions = select_box("Choose either keyword or hashtag to be searched", searchOptionsLists)
        write("Going ahead with ", searchOptions)
        if searchOptions == "Keyword":
            textRes = txt_input(searchOptions)
        else:
            textRes = txt_input(searchOptions)
        count = num_input("How many tweets needs to be scraped")
        startDate = date_selection("Start date")
        endDate = date_selection("End date")
        tweetList = twitter_search_scraper(searchOptions, textRes, count, startDate, endDate)
        pdDataFrame, stDataFrame = scrap_data(tweetList, columnnames)
        upload_to_mongodb(pdDataFrame, textRes, startDate, endDate)
    elif sidebarSelResult == "Display":
        retrieveValue = check_avl_data()
        display_data(retrieveValue)
    elif sidebarSelResult == "Download":
        retrieveValue = check_avl_data()
        download_data(retrieveValue)


pdData, stData = 0, 0
if __name__ == '__main__':
    main()
